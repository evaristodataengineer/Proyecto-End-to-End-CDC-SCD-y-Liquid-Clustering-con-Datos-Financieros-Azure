{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè¶ Notebook 01: Setup del Ambiente\n",
    "\n",
    "## Proyecto End-to-End: CDC, SCD y Liquid Clustering\n",
    "\n",
    "---\n",
    "\n",
    "### Objetivos de este notebook:\n",
    "1. Configurar el ambiente de Databricks\n",
    "2. Crear la base de datos del proyecto\n",
    "3. Crear las tablas Bronze con CDC y Liquid Clustering habilitado\n",
    "4. Generar datos sint√©ticos para el caso financiero\n",
    "\n",
    "### Requisitos:\n",
    "- Azure Databricks Runtime 15.2+ LTS (recomendado para Liquid Clustering GA)\n",
    "- Cluster configurado con autoscaling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Verificaci√≥n del Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar versi√≥n de Spark y Databricks Runtime\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Databricks Runtime: {spark.conf.get('spark.databricks.clusterUsageTags.sparkVersion', 'N/A')}\")\n",
    "\n",
    "# Verificar si Liquid Clustering est√° disponible (DBR 13.3+)\n",
    "try:\n",
    "    spark.sql(\"SELECT 1\").show()\n",
    "    print(\"‚úÖ Conexi√≥n a Spark exitosa\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuraci√≥n de Variables del Proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURACI√ìN DEL PROYECTO\n",
    "# ============================================\n",
    "\n",
    "# Nombre de la base de datos\n",
    "DATABASE_NAME = \"financial_lakehouse\"\n",
    "\n",
    "# Ubicaci√≥n de almacenamiento (ajustar seg√∫n tu configuraci√≥n)\n",
    "# Para Unity Catalog, usar: catalog.schema\n",
    "# Para Hive Metastore, usar path en DBFS o ADLS\n",
    "STORAGE_PATH = f\"/mnt/delta/{DATABASE_NAME}\"\n",
    "\n",
    "# Configuraci√≥n de tablas\n",
    "TABLES = {\n",
    "    \"bronze\": {\n",
    "        \"clientes\": \"bronze_clientes\",\n",
    "        \"cuentas\": \"bronze_cuentas\",\n",
    "        \"transacciones\": \"bronze_transacciones\"\n",
    "    },\n",
    "    \"silver\": {\n",
    "        \"clientes\": \"silver_dim_clientes\",\n",
    "        \"cuentas\": \"silver_dim_cuentas\",\n",
    "        \"transacciones\": \"silver_fact_transacciones\"\n",
    "    },\n",
    "    \"gold\": {\n",
    "        \"resumen_cliente\": \"gold_resumen_cliente\",\n",
    "        \"metricas_diarias\": \"gold_metricas_diarias\",\n",
    "        \"analisis_riesgo\": \"gold_analisis_riesgo\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"üìÅ Base de datos: {DATABASE_NAME}\")\n",
    "print(f\"üìÇ Storage path: {STORAGE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Configuraci√≥n de Spark para Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURACIONES DE SPARK PARA DELTA LAKE\n",
    "# ============================================\n",
    "\n",
    "# Habilitar Change Data Feed por defecto para nuevas tablas\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", \"true\")\n",
    "\n",
    "# Habilitar optimizaciones de escritura\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "\n",
    "# Habilitar auto-compactaci√≥n\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "\n",
    "# Configurar retention para historial (7 d√≠as)\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")\n",
    "\n",
    "# Mostrar configuraciones actuales\n",
    "print(\"‚öôÔ∏è Configuraciones de Delta Lake:\")\n",
    "print(f\"   CDC habilitado por defecto: {spark.conf.get('spark.databricks.delta.properties.defaults.enableChangeDataFeed')}\")\n",
    "print(f\"   Optimize Write: {spark.conf.get('spark.databricks.delta.optimizeWrite.enabled')}\")\n",
    "print(f\"   Auto Compact: {spark.conf.get('spark.databricks.delta.autoCompact.enabled')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Creaci√≥n de la Base de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CREAR BASE DE DATOS\n",
    "# ============================================\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\n",
    "    COMMENT 'Base de datos para proyecto de CDC, SCD y Liquid Clustering con datos financieros'\n",
    "\"\"\")\n",
    "\n",
    "# Usar la base de datos\n",
    "spark.sql(f\"USE {DATABASE_NAME}\")\n",
    "\n",
    "print(f\"‚úÖ Base de datos '{DATABASE_NAME}' creada y seleccionada\")\n",
    "\n",
    "# Verificar\n",
    "spark.sql(\"SELECT current_database()\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Creaci√≥n de Tablas Bronze con CDC y Liquid Clustering\n",
    "\n",
    "### üìå Caracter√≠sticas importantes:\n",
    "- **CLUSTER BY**: Habilita Liquid Clustering (reemplaza partitioning + Z-ORDER)\n",
    "- **enableChangeDataFeed**: Habilita CDC para capturar cambios\n",
    "- Las columnas de clustering se eligen seg√∫n patrones de consulta esperados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TABLA BRONZE: CLIENTES\n",
    "# ============================================\n",
    "# Clustering por: cliente_id (b√∫squedas frecuentes), fecha_ingesta (time-based queries)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {TABLES['bronze']['clientes']} (\n",
    "        -- Identificadores\n",
    "        cliente_id STRING NOT NULL COMMENT 'ID √∫nico del cliente',\n",
    "        \n",
    "        -- Datos personales\n",
    "        nombre STRING COMMENT 'Nombre completo del cliente',\n",
    "        email STRING COMMENT 'Correo electr√≥nico',\n",
    "        telefono STRING COMMENT 'N√∫mero de tel√©fono',\n",
    "        \n",
    "        -- Direcci√≥n\n",
    "        direccion STRING COMMENT 'Direcci√≥n completa',\n",
    "        ciudad STRING COMMENT 'Ciudad de residencia',\n",
    "        pais STRING COMMENT 'Pa√≠s de residencia',\n",
    "        codigo_postal STRING COMMENT 'C√≥digo postal',\n",
    "        \n",
    "        -- Datos adicionales\n",
    "        fecha_nacimiento DATE COMMENT 'Fecha de nacimiento',\n",
    "        genero STRING COMMENT 'G√©nero del cliente',\n",
    "        segmento_cliente STRING COMMENT 'Segmento: RETAIL, PREMIUM, VIP, CORPORATE',\n",
    "        estado STRING COMMENT 'Estado: ACTIVO, INACTIVO, SUSPENDIDO',\n",
    "        \n",
    "        -- Metadatos de ingesta\n",
    "        fecha_registro TIMESTAMP COMMENT 'Fecha de registro en sistema origen',\n",
    "        fecha_ingesta TIMESTAMP COMMENT 'Timestamp de ingesta a Bronze',\n",
    "        fuente STRING COMMENT 'Sistema origen de los datos',\n",
    "        operacion STRING COMMENT 'Tipo de operaci√≥n: INSERT, UPDATE, DELETE'\n",
    "    )\n",
    "    USING DELTA\n",
    "    CLUSTER BY (cliente_id, fecha_ingesta)\n",
    "    COMMENT 'Tabla Bronze de Clientes - Raw data con CDC habilitado'\n",
    "    TBLPROPERTIES (\n",
    "        'delta.enableChangeDataFeed' = 'true',\n",
    "        'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "        'delta.autoOptimize.autoCompact' = 'true',\n",
    "        'delta.logRetentionDuration' = 'interval 30 days',\n",
    "        'delta.deletedFileRetentionDuration' = 'interval 7 days'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Tabla {TABLES['bronze']['clientes']} creada con Liquid Clustering y CDC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TABLA BRONZE: CUENTAS BANCARIAS\n",
    "# ============================================\n",
    "# Clustering por: numero_cuenta (PK), cliente_id (FK frecuente)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {TABLES['bronze']['cuentas']} (\n",
    "        -- Identificadores\n",
    "        numero_cuenta STRING NOT NULL COMMENT 'N√∫mero √∫nico de cuenta',\n",
    "        cliente_id STRING NOT NULL COMMENT 'ID del cliente propietario',\n",
    "        \n",
    "        -- Informaci√≥n de la cuenta\n",
    "        tipo_cuenta STRING COMMENT 'Tipo: AHORRO, CORRIENTE, CREDITO, INVERSION',\n",
    "        moneda STRING COMMENT 'C√≥digo de moneda ISO (USD, EUR, PEN, etc.)',\n",
    "        saldo_actual DECIMAL(18,2) COMMENT 'Saldo actual de la cuenta',\n",
    "        saldo_disponible DECIMAL(18,2) COMMENT 'Saldo disponible para transacciones',\n",
    "        limite_credito DECIMAL(18,2) COMMENT 'L√≠mite de cr√©dito (si aplica)',\n",
    "        \n",
    "        -- Estado y fechas\n",
    "        estado STRING COMMENT 'Estado: ACTIVA, BLOQUEADA, CERRADA',\n",
    "        fecha_apertura DATE COMMENT 'Fecha de apertura de la cuenta',\n",
    "        fecha_ultimo_movimiento TIMESTAMP COMMENT 'Fecha del √∫ltimo movimiento',\n",
    "        \n",
    "        -- Informaci√≥n adicional\n",
    "        sucursal STRING COMMENT 'C√≥digo de sucursal',\n",
    "        ejecutivo_cuenta STRING COMMENT 'ID del ejecutivo asignado',\n",
    "        tasa_interes DECIMAL(5,4) COMMENT 'Tasa de inter√©s aplicable',\n",
    "        \n",
    "        -- Metadatos de ingesta\n",
    "        fecha_ingesta TIMESTAMP COMMENT 'Timestamp de ingesta a Bronze',\n",
    "        fuente STRING COMMENT 'Sistema origen',\n",
    "        operacion STRING COMMENT 'Tipo de operaci√≥n: INSERT, UPDATE, DELETE'\n",
    "    )\n",
    "    USING DELTA\n",
    "    CLUSTER BY (numero_cuenta, cliente_id)\n",
    "    COMMENT 'Tabla Bronze de Cuentas Bancarias - Raw data con CDC habilitado'\n",
    "    TBLPROPERTIES (\n",
    "        'delta.enableChangeDataFeed' = 'true',\n",
    "        'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "        'delta.autoOptimize.autoCompact' = 'true'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Tabla {TABLES['bronze']['cuentas']} creada con Liquid Clustering y CDC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TABLA BRONZE: TRANSACCIONES\n",
    "# ============================================\n",
    "# Clustering por: fecha_transaccion (time-series), numero_cuenta (joins frecuentes)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {TABLES['bronze']['transacciones']} (\n",
    "        -- Identificadores\n",
    "        transaccion_id STRING NOT NULL COMMENT 'ID √∫nico de la transacci√≥n',\n",
    "        numero_cuenta STRING NOT NULL COMMENT 'N√∫mero de cuenta asociada',\n",
    "        \n",
    "        -- Detalles de la transacci√≥n\n",
    "        tipo_transaccion STRING COMMENT 'Tipo: DEPOSITO, RETIRO, TRANSFERENCIA, PAGO, COMPRA',\n",
    "        monto DECIMAL(18,2) COMMENT 'Monto de la transacci√≥n',\n",
    "        moneda STRING COMMENT 'C√≥digo de moneda',\n",
    "        descripcion STRING COMMENT 'Descripci√≥n de la transacci√≥n',\n",
    "        \n",
    "        -- Informaci√≥n temporal\n",
    "        fecha_transaccion TIMESTAMP COMMENT 'Fecha y hora de la transacci√≥n',\n",
    "        fecha_liquidacion DATE COMMENT 'Fecha de liquidaci√≥n',\n",
    "        \n",
    "        -- Canal y ubicaci√≥n\n",
    "        canal STRING COMMENT 'Canal: ATM, ONLINE, MOBILE, SUCURSAL, POS',\n",
    "        ubicacion STRING COMMENT 'Ubicaci√≥n de la transacci√≥n',\n",
    "        dispositivo_id STRING COMMENT 'ID del dispositivo (si aplica)',\n",
    "        \n",
    "        -- Cuenta destino (para transferencias)\n",
    "        cuenta_destino STRING COMMENT 'Cuenta destino para transferencias',\n",
    "        banco_destino STRING COMMENT 'Banco destino para transferencias interbancarias',\n",
    "        \n",
    "        -- Estado y categorizaci√≥n\n",
    "        estado STRING COMMENT 'Estado: PENDIENTE, COMPLETADA, RECHAZADA, REVERSADA',\n",
    "        categoria STRING COMMENT 'Categor√≠a: ALIMENTOS, TRANSPORTE, SERVICIOS, etc.',\n",
    "        \n",
    "        -- Informaci√≥n de riesgo\n",
    "        score_fraude DECIMAL(5,2) COMMENT 'Score de riesgo de fraude (0-100)',\n",
    "        requiere_revision BOOLEAN COMMENT 'Flag si requiere revisi√≥n manual',\n",
    "        \n",
    "        -- Metadatos de ingesta\n",
    "        fecha_ingesta TIMESTAMP COMMENT 'Timestamp de ingesta a Bronze',\n",
    "        fuente STRING COMMENT 'Sistema origen',\n",
    "        operacion STRING COMMENT 'Tipo de operaci√≥n'\n",
    "    )\n",
    "    USING DELTA\n",
    "    CLUSTER BY (fecha_transaccion, numero_cuenta, tipo_transaccion)\n",
    "    COMMENT 'Tabla Bronze de Transacciones - Raw data con CDC habilitado'\n",
    "    TBLPROPERTIES (\n",
    "        'delta.enableChangeDataFeed' = 'true',\n",
    "        'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "        'delta.autoOptimize.autoCompact' = 'true',\n",
    "        'delta.logRetentionDuration' = 'interval 90 days'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Tabla {TABLES['bronze']['transacciones']} creada con Liquid Clustering y CDC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Verificaci√≥n de Tablas Creadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VERIFICAR TABLAS CREADAS\n",
    "# ============================================\n",
    "\n",
    "print(\"üìã Tablas en la base de datos:\")\n",
    "spark.sql(f\"SHOW TABLES IN {DATABASE_NAME}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar propiedades de Liquid Clustering y CDC\n",
    "for layer, tables in TABLES.items():\n",
    "    if layer == \"bronze\":\n",
    "        for table_name, table_full_name in tables.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üìä Detalles de: {table_full_name}\")\n",
    "            print('='*60)\n",
    "            \n",
    "            # Mostrar propiedades de la tabla\n",
    "            spark.sql(f\"DESCRIBE DETAIL {table_full_name}\").show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar propiedades espec√≠ficas de la tabla (CDC y Clustering)\n",
    "print(\"\\nüìù Propiedades de tabla Bronze Clientes:\")\n",
    "spark.sql(f\"SHOW TBLPROPERTIES {TABLES['bronze']['clientes']}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Generaci√≥n de Datos Sint√©ticos\n",
    "\n",
    "Generaremos datos de prueba para simular un sistema bancario real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, lit, expr, rand, randn, floor, ceil,\n",
    "    date_add, date_sub, current_timestamp, current_date,\n",
    "    concat, lpad, when, round as spark_round,\n",
    "    to_timestamp, to_date, unix_timestamp,\n",
    "    array, element_at, shuffle\n",
    ")\n",
    "from pyspark.sql.types import *\n",
    "import random\n",
    "\n",
    "# Configuraci√≥n de generaci√≥n de datos\n",
    "NUM_CLIENTES = 1000\n",
    "NUM_CUENTAS = 1500  # Algunos clientes tienen m√∫ltiples cuentas\n",
    "NUM_TRANSACCIONES = 10000\n",
    "\n",
    "print(f\"üîÑ Generando datos sint√©ticos:\")\n",
    "print(f\"   - Clientes: {NUM_CLIENTES:,}\")\n",
    "print(f\"   - Cuentas: {NUM_CUENTAS:,}\")\n",
    "print(f\"   - Transacciones: {NUM_TRANSACCIONES:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GENERAR DATOS DE CLIENTES\n",
    "# ============================================\n",
    "\n",
    "# Arrays para datos aleatorios\n",
    "nombres = [\"Juan\", \"Mar√≠a\", \"Carlos\", \"Ana\", \"Pedro\", \"Laura\", \"Miguel\", \"Sofia\", \"Diego\", \"Valentina\",\n",
    "           \"Andr√©s\", \"Camila\", \"Roberto\", \"Isabella\", \"Fernando\", \"Luc√≠a\", \"Gabriel\", \"Paula\", \"Daniel\", \"Martina\"]\n",
    "apellidos = [\"Garc√≠a\", \"Rodr√≠guez\", \"Mart√≠nez\", \"L√≥pez\", \"Gonz√°lez\", \"Hern√°ndez\", \"P√©rez\", \"S√°nchez\", \n",
    "             \"Ram√≠rez\", \"Torres\", \"Flores\", \"Rivera\", \"G√≥mez\", \"D√≠az\", \"Reyes\", \"Morales\", \"Ortiz\", \"Cruz\"]\n",
    "ciudades = [\"Lima\", \"Arequipa\", \"Trujillo\", \"Chiclayo\", \"Piura\", \"Cusco\", \"Huancayo\", \"Iquitos\", \"Tacna\", \"Puno\"]\n",
    "segmentos = [\"RETAIL\", \"PREMIUM\", \"VIP\", \"CORPORATE\"]\n",
    "estados = [\"ACTIVO\", \"ACTIVO\", \"ACTIVO\", \"ACTIVO\", \"INACTIVO\"]  # 80% activos\n",
    "\n",
    "# Crear DataFrame base\n",
    "clientes_df = spark.range(1, NUM_CLIENTES + 1).select(\n",
    "    # ID del cliente\n",
    "    concat(lit(\"CLI-\"), lpad(col(\"id\").cast(\"string\"), 8, \"0\")).alias(\"cliente_id\"),\n",
    "    \n",
    "    # Nombre completo\n",
    "    concat(\n",
    "        element_at(array(*[lit(n) for n in nombres]), (floor(rand() * len(nombres)) + 1).cast(\"int\")),\n",
    "        lit(\" \"),\n",
    "        element_at(array(*[lit(a) for a in apellidos]), (floor(rand() * len(apellidos)) + 1).cast(\"int\")),\n",
    "        lit(\" \"),\n",
    "        element_at(array(*[lit(a) for a in apellidos]), (floor(rand() * len(apellidos)) + 1).cast(\"int\"))\n",
    "    ).alias(\"nombre\"),\n",
    "    \n",
    "    # Email\n",
    "    concat(\n",
    "        lit(\"cliente\"),\n",
    "        col(\"id\").cast(\"string\"),\n",
    "        lit(\"@\"),\n",
    "        element_at(array(lit(\"gmail.com\"), lit(\"hotmail.com\"), lit(\"yahoo.com\"), lit(\"outlook.com\")), \n",
    "                  (floor(rand() * 4) + 1).cast(\"int\"))\n",
    "    ).alias(\"email\"),\n",
    "    \n",
    "    # Tel√©fono\n",
    "    concat(lit(\"+51 9\"), lpad((floor(rand() * 99999999) + 10000000).cast(\"string\"), 8, \"0\")).alias(\"telefono\"),\n",
    "    \n",
    "    # Direcci√≥n\n",
    "    concat(\n",
    "        lit(\"Calle \"),\n",
    "        (floor(rand() * 100) + 1).cast(\"string\"),\n",
    "        lit(\" #\"),\n",
    "        (floor(rand() * 999) + 1).cast(\"string\")\n",
    "    ).alias(\"direccion\"),\n",
    "    \n",
    "    # Ciudad\n",
    "    element_at(array(*[lit(c) for c in ciudades]), (floor(rand() * len(ciudades)) + 1).cast(\"int\")).alias(\"ciudad\"),\n",
    "    \n",
    "    # Pa√≠s\n",
    "    lit(\"Per√∫\").alias(\"pais\"),\n",
    "    \n",
    "    # C√≥digo postal\n",
    "    lpad((floor(rand() * 99999) + 10000).cast(\"string\"), 5, \"0\").alias(\"codigo_postal\"),\n",
    "    \n",
    "    # Fecha de nacimiento (entre 18 y 70 a√±os)\n",
    "    date_sub(current_date(), (floor(rand() * 52 * 365) + 18 * 365).cast(\"int\")).alias(\"fecha_nacimiento\"),\n",
    "    \n",
    "    # G√©nero\n",
    "    when(rand() < 0.5, lit(\"M\")).otherwise(lit(\"F\")).alias(\"genero\"),\n",
    "    \n",
    "    # Segmento\n",
    "    element_at(array(*[lit(s) for s in segmentos]), (floor(rand() * len(segmentos)) + 1).cast(\"int\")).alias(\"segmento_cliente\"),\n",
    "    \n",
    "    # Estado\n",
    "    element_at(array(*[lit(e) for e in estados]), (floor(rand() * len(estados)) + 1).cast(\"int\")).alias(\"estado\"),\n",
    "    \n",
    "    # Fecha de registro (√∫ltimos 5 a√±os)\n",
    "    to_timestamp(date_sub(current_date(), (floor(rand() * 5 * 365)).cast(\"int\"))).alias(\"fecha_registro\"),\n",
    "    \n",
    "    # Metadatos de ingesta\n",
    "    current_timestamp().alias(\"fecha_ingesta\"),\n",
    "    lit(\"CORE_BANKING\").alias(\"fuente\"),\n",
    "    lit(\"INSERT\").alias(\"operacion\")\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataFrame de clientes generado: {clientes_df.count()} registros\")\n",
    "clientes_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INSERTAR DATOS EN BRONZE_CLIENTES\n",
    "# ============================================\n",
    "\n",
    "clientes_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(TABLES['bronze']['clientes'])\n",
    "\n",
    "print(f\"‚úÖ Datos insertados en {TABLES['bronze']['clientes']}\")\n",
    "\n",
    "# Verificar conteo\n",
    "count = spark.table(TABLES['bronze']['clientes']).count()\n",
    "print(f\"üìä Total de registros: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GENERAR DATOS DE CUENTAS BANCARIAS\n",
    "# ============================================\n",
    "\n",
    "tipos_cuenta = [\"AHORRO\", \"CORRIENTE\", \"CREDITO\", \"INVERSION\"]\n",
    "monedas = [\"PEN\", \"PEN\", \"PEN\", \"USD\"]  # 75% en soles\n",
    "estados_cuenta = [\"ACTIVA\", \"ACTIVA\", \"ACTIVA\", \"ACTIVA\", \"BLOQUEADA\"]  # 80% activas\n",
    "sucursales = [\"SUC-001\", \"SUC-002\", \"SUC-003\", \"SUC-004\", \"SUC-005\", \"SUC-010\", \"SUC-015\", \"SUC-020\"]\n",
    "\n",
    "# Obtener IDs de clientes existentes\n",
    "clientes_ids = spark.table(TABLES['bronze']['clientes']).select(\"cliente_id\").collect()\n",
    "clientes_list = [row.cliente_id for row in clientes_ids]\n",
    "\n",
    "cuentas_df = spark.range(1, NUM_CUENTAS + 1).select(\n",
    "    # N√∫mero de cuenta\n",
    "    concat(lit(\"191-\"), lpad(col(\"id\").cast(\"string\"), 10, \"0\")).alias(\"numero_cuenta\"),\n",
    "    \n",
    "    # Cliente ID (distribuido entre clientes existentes)\n",
    "    element_at(\n",
    "        array(*[lit(c) for c in clientes_list]),\n",
    "        (floor(rand() * len(clientes_list)) + 1).cast(\"int\")\n",
    "    ).alias(\"cliente_id\"),\n",
    "    \n",
    "    # Tipo de cuenta\n",
    "    element_at(array(*[lit(t) for t in tipos_cuenta]), (floor(rand() * len(tipos_cuenta)) + 1).cast(\"int\")).alias(\"tipo_cuenta\"),\n",
    "    \n",
    "    # Moneda\n",
    "    element_at(array(*[lit(m) for m in monedas]), (floor(rand() * len(monedas)) + 1).cast(\"int\")).alias(\"moneda\"),\n",
    "    \n",
    "    # Saldo actual (entre 0 y 500,000)\n",
    "    spark_round(rand() * 500000, 2).alias(\"saldo_actual\"),\n",
    "    \n",
    "    # Saldo disponible (90-100% del saldo actual)\n",
    "    spark_round(rand() * 500000 * (0.9 + rand() * 0.1), 2).alias(\"saldo_disponible\"),\n",
    "    \n",
    "    # L√≠mite de cr√©dito (solo para cuentas de cr√©dito)\n",
    "    when(\n",
    "        element_at(array(*[lit(t) for t in tipos_cuenta]), (floor(rand() * len(tipos_cuenta)) + 1).cast(\"int\")) == \"CREDITO\",\n",
    "        spark_round(rand() * 50000 + 5000, 2)\n",
    "    ).otherwise(lit(None)).alias(\"limite_credito\"),\n",
    "    \n",
    "    # Estado\n",
    "    element_at(array(*[lit(e) for e in estados_cuenta]), (floor(rand() * len(estados_cuenta)) + 1).cast(\"int\")).alias(\"estado\"),\n",
    "    \n",
    "    # Fecha de apertura\n",
    "    date_sub(current_date(), (floor(rand() * 3 * 365)).cast(\"int\")).alias(\"fecha_apertura\"),\n",
    "    \n",
    "    # √öltimo movimiento\n",
    "    to_timestamp(date_sub(current_date(), (floor(rand() * 30)).cast(\"int\"))).alias(\"fecha_ultimo_movimiento\"),\n",
    "    \n",
    "    # Sucursal\n",
    "    element_at(array(*[lit(s) for s in sucursales]), (floor(rand() * len(sucursales)) + 1).cast(\"int\")).alias(\"sucursal\"),\n",
    "    \n",
    "    # Ejecutivo\n",
    "    concat(lit(\"EJE-\"), lpad((floor(rand() * 50) + 1).cast(\"string\"), 3, \"0\")).alias(\"ejecutivo_cuenta\"),\n",
    "    \n",
    "    # Tasa de inter√©s (1-15%)\n",
    "    spark_round(rand() * 0.14 + 0.01, 4).alias(\"tasa_interes\"),\n",
    "    \n",
    "    # Metadatos\n",
    "    current_timestamp().alias(\"fecha_ingesta\"),\n",
    "    lit(\"CORE_BANKING\").alias(\"fuente\"),\n",
    "    lit(\"INSERT\").alias(\"operacion\")\n",
    ")\n",
    "\n",
    "# Insertar en tabla Bronze\n",
    "cuentas_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(TABLES['bronze']['cuentas'])\n",
    "\n",
    "print(f\"‚úÖ Datos insertados en {TABLES['bronze']['cuentas']}\")\n",
    "print(f\"üìä Total de registros: {spark.table(TABLES['bronze']['cuentas']).count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GENERAR DATOS DE TRANSACCIONES\n",
    "# ============================================\n",
    "\n",
    "tipos_transaccion = [\"DEPOSITO\", \"RETIRO\", \"TRANSFERENCIA\", \"PAGO\", \"COMPRA\"]\n",
    "canales = [\"ATM\", \"ONLINE\", \"MOBILE\", \"SUCURSAL\", \"POS\"]\n",
    "estados_tx = [\"COMPLETADA\", \"COMPLETADA\", \"COMPLETADA\", \"COMPLETADA\", \"PENDIENTE\", \"RECHAZADA\"]\n",
    "categorias = [\"ALIMENTOS\", \"TRANSPORTE\", \"SERVICIOS\", \"ENTRETENIMIENTO\", \"SALUD\", \"EDUCACION\", \"OTROS\"]\n",
    "\n",
    "# Obtener cuentas existentes\n",
    "cuentas_ids = spark.table(TABLES['bronze']['cuentas']).select(\"numero_cuenta\").collect()\n",
    "cuentas_list = [row.numero_cuenta for row in cuentas_ids]\n",
    "\n",
    "transacciones_df = spark.range(1, NUM_TRANSACCIONES + 1).select(\n",
    "    # ID de transacci√≥n\n",
    "    concat(lit(\"TXN-\"), lpad(col(\"id\").cast(\"string\"), 12, \"0\")).alias(\"transaccion_id\"),\n",
    "    \n",
    "    # Cuenta asociada\n",
    "    element_at(\n",
    "        array(*[lit(c) for c in cuentas_list]),\n",
    "        (floor(rand() * len(cuentas_list)) + 1).cast(\"int\")\n",
    "    ).alias(\"numero_cuenta\"),\n",
    "    \n",
    "    # Tipo de transacci√≥n\n",
    "    element_at(array(*[lit(t) for t in tipos_transaccion]), (floor(rand() * len(tipos_transaccion)) + 1).cast(\"int\")).alias(\"tipo_transaccion\"),\n",
    "    \n",
    "    # Monto (entre 1 y 50,000)\n",
    "    spark_round(rand() * 49999 + 1, 2).alias(\"monto\"),\n",
    "    \n",
    "    # Moneda\n",
    "    element_at(array(lit(\"PEN\"), lit(\"PEN\"), lit(\"PEN\"), lit(\"USD\")), (floor(rand() * 4) + 1).cast(\"int\")).alias(\"moneda\"),\n",
    "    \n",
    "    # Descripci√≥n\n",
    "    concat(\n",
    "        lit(\"Transacci√≥n #\"),\n",
    "        col(\"id\").cast(\"string\")\n",
    "    ).alias(\"descripcion\"),\n",
    "    \n",
    "    # Fecha de transacci√≥n (√∫ltimos 90 d√≠as)\n",
    "    to_timestamp(\n",
    "        concat(\n",
    "            date_sub(current_date(), (floor(rand() * 90)).cast(\"int\")).cast(\"string\"),\n",
    "            lit(\" \"),\n",
    "            lpad((floor(rand() * 24)).cast(\"string\"), 2, \"0\"),\n",
    "            lit(\":\"),\n",
    "            lpad((floor(rand() * 60)).cast(\"string\"), 2, \"0\"),\n",
    "            lit(\":\"),\n",
    "            lpad((floor(rand() * 60)).cast(\"string\"), 2, \"0\")\n",
    "        )\n",
    "    ).alias(\"fecha_transaccion\"),\n",
    "    \n",
    "    # Fecha de liquidaci√≥n\n",
    "    date_sub(current_date(), (floor(rand() * 88)).cast(\"int\")).alias(\"fecha_liquidacion\"),\n",
    "    \n",
    "    # Canal\n",
    "    element_at(array(*[lit(c) for c in canales]), (floor(rand() * len(canales)) + 1).cast(\"int\")).alias(\"canal\"),\n",
    "    \n",
    "    # Ubicaci√≥n\n",
    "    element_at(array(*[lit(c) for c in ciudades]), (floor(rand() * len(ciudades)) + 1).cast(\"int\")).alias(\"ubicacion\"),\n",
    "    \n",
    "    # Dispositivo\n",
    "    concat(lit(\"DEV-\"), lpad((floor(rand() * 10000)).cast(\"string\"), 6, \"0\")).alias(\"dispositivo_id\"),\n",
    "    \n",
    "    # Cuenta destino (para transferencias)\n",
    "    when(rand() < 0.2, \n",
    "         concat(lit(\"191-\"), lpad((floor(rand() * 9999999999) + 1).cast(\"string\"), 10, \"0\"))\n",
    "    ).otherwise(lit(None)).alias(\"cuenta_destino\"),\n",
    "    \n",
    "    # Banco destino\n",
    "    when(rand() < 0.1,\n",
    "         element_at(array(lit(\"BCP\"), lit(\"BBVA\"), lit(\"INTERBANK\"), lit(\"SCOTIABANK\")), (floor(rand() * 4) + 1).cast(\"int\"))\n",
    "    ).otherwise(lit(None)).alias(\"banco_destino\"),\n",
    "    \n",
    "    # Estado\n",
    "    element_at(array(*[lit(e) for e in estados_tx]), (floor(rand() * len(estados_tx)) + 1).cast(\"int\")).alias(\"estado\"),\n",
    "    \n",
    "    # Categor√≠a\n",
    "    element_at(array(*[lit(c) for c in categorias]), (floor(rand() * len(categorias)) + 1).cast(\"int\")).alias(\"categoria\"),\n",
    "    \n",
    "    # Score de fraude (0-100)\n",
    "    spark_round(rand() * 100, 2).alias(\"score_fraude\"),\n",
    "    \n",
    "    # Requiere revisi√≥n (si score > 80)\n",
    "    (rand() * 100 > 80).alias(\"requiere_revision\"),\n",
    "    \n",
    "    # Metadatos\n",
    "    current_timestamp().alias(\"fecha_ingesta\"),\n",
    "    lit(\"TRANSACTION_SYSTEM\").alias(\"fuente\"),\n",
    "    lit(\"INSERT\").alias(\"operacion\")\n",
    ")\n",
    "\n",
    "# Insertar en tabla Bronze\n",
    "transacciones_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(TABLES['bronze']['transacciones'])\n",
    "\n",
    "print(f\"‚úÖ Datos insertados en {TABLES['bronze']['transacciones']}\")\n",
    "print(f\"üìä Total de registros: {spark.table(TABLES['bronze']['transacciones']).count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Verificaci√≥n Final del Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RESUMEN DEL SETUP\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä RESUMEN DEL SETUP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for table_name in [TABLES['bronze']['clientes'], TABLES['bronze']['cuentas'], TABLES['bronze']['transacciones']]:\n",
    "    count = spark.table(table_name).count()\n",
    "    print(f\"\\nüìÅ {table_name}:\")\n",
    "    print(f\"   ‚îî‚îÄ Registros: {count:,}\")\n",
    "    \n",
    "    # Verificar CDC\n",
    "    props = spark.sql(f\"SHOW TBLPROPERTIES {table_name}\").filter(\"key = 'delta.enableChangeDataFeed'\").collect()\n",
    "    if props:\n",
    "        print(f\"   ‚îî‚îÄ CDC habilitado: {props[0]['value']}\")\n",
    "    \n",
    "    # Verificar Clustering\n",
    "    detail = spark.sql(f\"DESCRIBE DETAIL {table_name}\").collect()[0]\n",
    "    if detail.clusteringColumns:\n",
    "        print(f\"   ‚îî‚îÄ Liquid Clustering: {detail.clusteringColumns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que CDC est√° capturando cambios (historial)\n",
    "print(\"\\nüìú Historial de cambios en Bronze Clientes:\")\n",
    "spark.sql(f\"DESCRIBE HISTORY {TABLES['bronze']['clientes']} LIMIT 5\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Setup Completado\n",
    "\n",
    "### Lo que hemos configurado:\n",
    "\n",
    "1. ‚úÖ Base de datos `financial_lakehouse` creada\n",
    "2. ‚úÖ Tablas Bronze con **Liquid Clustering** habilitado\n",
    "3. ‚úÖ **Change Data Feed (CDC)** habilitado en todas las tablas\n",
    "4. ‚úÖ Datos sint√©ticos cargados para pruebas\n",
    "\n",
    "### Pr√≥ximo paso:\n",
    "Continuar con el **Notebook 02: Bronze Layer - CDC** para ver c√≥mo capturar y procesar cambios incrementales.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

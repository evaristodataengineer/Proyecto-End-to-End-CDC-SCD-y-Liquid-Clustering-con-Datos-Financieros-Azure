{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè¶ Notebook 02: Bronze Layer - Change Data Capture (CDC)\n",
    "\n",
    "## Capturando Cambios Incrementales en Delta Lake\n",
    "\n",
    "---\n",
    "\n",
    "### Objetivos de este notebook:\n",
    "1. Entender c√≥mo funciona Change Data Feed (CDF) en Delta Lake\n",
    "2. Simular operaciones de INSERT, UPDATE y DELETE\n",
    "3. Leer el Change Data Feed en modo batch y streaming\n",
    "4. Procesar cambios incrementales para la capa Silver\n",
    "\n",
    "### Conceptos clave:\n",
    "- **Change Data Feed (CDF)**: Caracter√≠stica de Delta Lake que registra cambios a nivel de fila\n",
    "- **_change_type**: Columna metadata que indica el tipo de cambio (insert, update_preimage, update_postimage, delete)\n",
    "- **_commit_version**: Versi√≥n del commit donde ocurri√≥ el cambio\n",
    "- **_commit_timestamp**: Timestamp del commit\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Configuraci√≥n Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Configuraci√≥n\n",
    "DATABASE_NAME = \"financial_lakehouse\"\n",
    "spark.sql(f\"USE {DATABASE_NAME}\")\n",
    "\n",
    "# Nombres de tablas\n",
    "BRONZE_CLIENTES = \"bronze_clientes\"\n",
    "BRONZE_CUENTAS = \"bronze_cuentas\"\n",
    "BRONZE_TRANSACCIONES = \"bronze_transacciones\"\n",
    "\n",
    "print(f\"‚úÖ Usando base de datos: {DATABASE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Verificar CDC Habilitado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VERIFICAR QUE CDC EST√Å HABILITADO\n",
    "# ============================================\n",
    "\n",
    "def check_cdf_enabled(table_name):\n",
    "    \"\"\"Verifica si Change Data Feed est√° habilitado en una tabla\"\"\"\n",
    "    props = spark.sql(f\"SHOW TBLPROPERTIES {table_name}\")\n",
    "    cdf_prop = props.filter(col(\"key\") == \"delta.enableChangeDataFeed\").collect()\n",
    "    \n",
    "    if cdf_prop and cdf_prop[0][\"value\"] == \"true\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "for table in [BRONZE_CLIENTES, BRONZE_CUENTAS, BRONZE_TRANSACCIONES]:\n",
    "    status = \"‚úÖ Habilitado\" if check_cdf_enabled(table) else \"‚ùå No habilitado\"\n",
    "    print(f\"{table}: CDC {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si alguna tabla no tiene CDC habilitado, habilitarlo\n",
    "def enable_cdf(table_name):\n",
    "    \"\"\"Habilita Change Data Feed en una tabla existente\"\"\"\n",
    "    spark.sql(f\"\"\"\n",
    "        ALTER TABLE {table_name} \n",
    "        SET TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')\n",
    "    \"\"\")\n",
    "    print(f\"‚úÖ CDC habilitado en {table_name}\")\n",
    "\n",
    "# Ejemplo: enable_cdf(BRONZE_CLIENTES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Simular Operaciones de Cambio\n",
    "\n",
    "Vamos a simular diferentes operaciones para generar el Change Data Feed:\n",
    "- **INSERT**: Nuevos clientes\n",
    "- **UPDATE**: Cambios en datos de clientes existentes\n",
    "- **DELETE**: Eliminaci√≥n de clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OBTENER VERSI√ìN ACTUAL ANTES DE LOS CAMBIOS\n",
    "# ============================================\n",
    "\n",
    "# Obtener la versi√≥n actual de la tabla\n",
    "history = spark.sql(f\"DESCRIBE HISTORY {BRONZE_CLIENTES} LIMIT 1\").collect()\n",
    "version_before = history[0][\"version\"]\n",
    "print(f\"üìå Versi√≥n actual de {BRONZE_CLIENTES}: {version_before}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Simular INSERTs (Nuevos Clientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INSERTAR NUEVOS CLIENTES\n",
    "# ============================================\n",
    "\n",
    "nuevos_clientes = [\n",
    "    (\"CLI-99990001\", \"Roberto Nuevocliente\", \"roberto.nuevo@gmail.com\", \"+51 999000001\", \n",
    "     \"Av. Nueva 123\", \"Lima\", \"Per√∫\", \"15001\", \"1985-03-15\", \"M\", \"VIP\", \"ACTIVO\",\n",
    "     \"2024-01-15 10:30:00\", \"CORE_BANKING\", \"INSERT\"),\n",
    "    (\"CLI-99990002\", \"Carolina Cliente\", \"carolina.cliente@hotmail.com\", \"+51 999000002\",\n",
    "     \"Jr. Reciente 456\", \"Arequipa\", \"Per√∫\", \"04001\", \"1990-07-22\", \"F\", \"PREMIUM\", \"ACTIVO\",\n",
    "     \"2024-01-16 14:45:00\", \"CORE_BANKING\", \"INSERT\"),\n",
    "    (\"CLI-99990003\", \"Fernando Fresco\", \"fernando.fresco@yahoo.com\", \"+51 999000003\",\n",
    "     \"Calle Actual 789\", \"Trujillo\", \"Per√∫\", \"13001\", \"1978-11-30\", \"M\", \"CORPORATE\", \"ACTIVO\",\n",
    "     \"2024-01-17 09:15:00\", \"MOBILE_APP\", \"INSERT\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"cliente_id\", StringType(), False),\n",
    "    StructField(\"nombre\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"telefono\", StringType(), True),\n",
    "    StructField(\"direccion\", StringType(), True),\n",
    "    StructField(\"ciudad\", StringType(), True),\n",
    "    StructField(\"pais\", StringType(), True),\n",
    "    StructField(\"codigo_postal\", StringType(), True),\n",
    "    StructField(\"fecha_nacimiento\", StringType(), True),\n",
    "    StructField(\"genero\", StringType(), True),\n",
    "    StructField(\"segmento_cliente\", StringType(), True),\n",
    "    StructField(\"estado\", StringType(), True),\n",
    "    StructField(\"fecha_registro\", StringType(), True),\n",
    "    StructField(\"fuente\", StringType(), True),\n",
    "    StructField(\"operacion\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_nuevos = spark.createDataFrame(nuevos_clientes, schema) \\\n",
    "    .withColumn(\"fecha_nacimiento\", to_date(col(\"fecha_nacimiento\"))) \\\n",
    "    .withColumn(\"fecha_registro\", to_timestamp(col(\"fecha_registro\"))) \\\n",
    "    .withColumn(\"fecha_ingesta\", current_timestamp())\n",
    "\n",
    "# Insertar nuevos clientes\n",
    "df_nuevos.write.format(\"delta\").mode(\"append\").saveAsTable(BRONZE_CLIENTES)\n",
    "\n",
    "print(f\"‚úÖ Insertados {df_nuevos.count()} nuevos clientes\")\n",
    "df_nuevos.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Simular UPDATEs (Cambios en Clientes Existentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ACTUALIZAR CLIENTES EXISTENTES\n",
    "# ============================================\n",
    "\n",
    "# Obtener DeltaTable para hacer MERGE/UPDATE\n",
    "delta_clientes = DeltaTable.forName(spark, BRONZE_CLIENTES)\n",
    "\n",
    "# Escenario 1: Cliente cambia de direcci√≥n (generar√° update_preimage y update_postimage)\n",
    "# Actualizamos uno de los clientes que acabamos de insertar\n",
    "spark.sql(f\"\"\"\n",
    "    UPDATE {BRONZE_CLIENTES}\n",
    "    SET \n",
    "        direccion = 'Av. Actualizada 999',\n",
    "        ciudad = 'Cusco',\n",
    "        codigo_postal = '08001',\n",
    "        operacion = 'UPDATE',\n",
    "        fecha_ingesta = current_timestamp()\n",
    "    WHERE cliente_id = 'CLI-99990001'\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Cliente CLI-99990001 actualizado (cambio de direcci√≥n)\")\n",
    "\n",
    "# Escenario 2: Cliente cambia de segmento (upgrade a VIP)\n",
    "spark.sql(f\"\"\"\n",
    "    UPDATE {BRONZE_CLIENTES}\n",
    "    SET \n",
    "        segmento_cliente = 'VIP',\n",
    "        operacion = 'UPDATE',\n",
    "        fecha_ingesta = current_timestamp()\n",
    "    WHERE cliente_id = 'CLI-99990002'\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Cliente CLI-99990002 actualizado (upgrade a VIP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Simular DELETEs (Eliminaci√≥n de Clientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ELIMINAR CLIENTE\n",
    "# ============================================\n",
    "\n",
    "# Soft delete (cambiar estado) - M√°s com√∫n en sistemas financieros\n",
    "spark.sql(f\"\"\"\n",
    "    UPDATE {BRONZE_CLIENTES}\n",
    "    SET \n",
    "        estado = 'INACTIVO',\n",
    "        operacion = 'SOFT_DELETE',\n",
    "        fecha_ingesta = current_timestamp()\n",
    "    WHERE cliente_id = 'CLI-99990003'\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Cliente CLI-99990003 marcado como INACTIVO (soft delete)\")\n",
    "\n",
    "# Hard delete (eliminar f√≠sicamente) - Menos com√∫n pero genera evento DELETE en CDF\n",
    "# spark.sql(f\"DELETE FROM {BRONZE_CLIENTES} WHERE cliente_id = 'CLI-99990003'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Leer Change Data Feed (Modo Batch)\n",
    "\n",
    "Ahora vamos a leer los cambios que acabamos de hacer usando el Change Data Feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LEER CDF - MODO BATCH (POR VERSI√ìN)\n",
    "# ============================================\n",
    "\n",
    "# Leer cambios desde la versi√≥n que guardamos antes\n",
    "changes_df = spark.read.format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", version_before + 1) \\\n",
    "    .table(BRONZE_CLIENTES)\n",
    "\n",
    "print(f\"üìä Cambios capturados desde versi√≥n {version_before + 1}:\")\n",
    "print(f\"   Total de registros de cambio: {changes_df.count()}\")\n",
    "\n",
    "# Mostrar los cambios con columnas metadata de CDC\n",
    "changes_df.select(\n",
    "    \"cliente_id\",\n",
    "    \"nombre\",\n",
    "    \"direccion\",\n",
    "    \"ciudad\",\n",
    "    \"segmento_cliente\",\n",
    "    \"estado\",\n",
    "    \"_change_type\",\n",
    "    \"_commit_version\",\n",
    "    \"_commit_timestamp\"\n",
    ").orderBy(\"cliente_id\", \"_commit_version\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ANALIZAR TIPOS DE CAMBIO\n",
    "# ============================================\n",
    "\n",
    "print(\"üìà Resumen de tipos de cambio:\")\n",
    "changes_df.groupBy(\"_change_type\").count().show()\n",
    "\n",
    "# Explicaci√≥n de los tipos de cambio:\n",
    "print(\"\"\"\n",
    "üìö Tipos de cambio en Change Data Feed:\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ _change_type        ‚îÇ Descripci√≥n                                       ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ insert              ‚îÇ Nueva fila insertada                              ‚îÇ\n",
    "‚îÇ update_preimage     ‚îÇ Valor ANTES de la actualizaci√≥n                   ‚îÇ\n",
    "‚îÇ update_postimage    ‚îÇ Valor DESPU√âS de la actualizaci√≥n                 ‚îÇ\n",
    "‚îÇ delete              ‚îÇ Fila eliminada                                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VER DETALLE DE UN UPDATE (PRE vs POST IMAGE)\n",
    "# ============================================\n",
    "\n",
    "print(\"üîÑ Detalle del UPDATE para cliente CLI-99990001:\")\n",
    "\n",
    "update_detail = changes_df.filter(\n",
    "    (col(\"cliente_id\") == \"CLI-99990001\") & \n",
    "    (col(\"_change_type\").isin([\"update_preimage\", \"update_postimage\"]))\n",
    ")\n",
    "\n",
    "update_detail.select(\n",
    "    \"cliente_id\",\n",
    "    \"direccion\",\n",
    "    \"ciudad\",\n",
    "    \"codigo_postal\",\n",
    "    \"_change_type\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Leer Change Data Feed (Modo Streaming)\n",
    "\n",
    "Para pipelines en tiempo real, podemos leer el CDF como un stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LEER CDF - MODO STREAMING\n",
    "# ============================================\n",
    "\n",
    "# Configurar stream desde el CDF\n",
    "stream_changes = spark.readStream.format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", 0) \\\n",
    "    .table(BRONZE_CLIENTES)\n",
    "\n",
    "print(\"‚úÖ Stream de CDC configurado\")\n",
    "print(f\"üìä Schema del stream:\")\n",
    "stream_changes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PROCESAR STREAM A TABLA DE AUDITOR√çA\n",
    "# ============================================\n",
    "\n",
    "# Crear tabla de auditor√≠a para CDC\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS audit_cdc_clientes (\n",
    "        cliente_id STRING,\n",
    "        nombre STRING,\n",
    "        email STRING,\n",
    "        direccion STRING,\n",
    "        ciudad STRING,\n",
    "        segmento_cliente STRING,\n",
    "        estado STRING,\n",
    "        change_type STRING,\n",
    "        commit_version LONG,\n",
    "        commit_timestamp TIMESTAMP,\n",
    "        processed_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    CLUSTER BY (cliente_id, commit_timestamp)\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Tabla de auditor√≠a creada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar el stream y escribir a la tabla de auditor√≠a\n",
    "# NOTA: En un ambiente real, esto correr√≠a continuamente\n",
    "\n",
    "audit_stream = stream_changes.select(\n",
    "    col(\"cliente_id\"),\n",
    "    col(\"nombre\"),\n",
    "    col(\"email\"),\n",
    "    col(\"direccion\"),\n",
    "    col(\"ciudad\"),\n",
    "    col(\"segmento_cliente\"),\n",
    "    col(\"estado\"),\n",
    "    col(\"_change_type\").alias(\"change_type\"),\n",
    "    col(\"_commit_version\").alias(\"commit_version\"),\n",
    "    col(\"_commit_timestamp\").alias(\"commit_timestamp\"),\n",
    "    current_timestamp().alias(\"processed_timestamp\")\n",
    ")\n",
    "\n",
    "# Escribir con trigger once (procesar todo lo disponible y parar)\n",
    "query = audit_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint/audit_cdc_clientes\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(\"audit_cdc_clientes\")\n",
    "\n",
    "# Esperar a que termine\n",
    "query.awaitTermination()\n",
    "\n",
    "print(\"‚úÖ Stream procesado exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver el resultado en la tabla de auditor√≠a\n",
    "print(\"üìã Tabla de Auditor√≠a CDC:\")\n",
    "spark.table(\"audit_cdc_clientes\") \\\n",
    "    .orderBy(col(\"commit_version\").desc(), \"cliente_id\") \\\n",
    "    .show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Casos de Uso Pr√°cticos del CDC\n",
    "\n",
    "### 6.1 Detectar Cambios Espec√≠ficos (Alertas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CASO 1: DETECTAR CAMBIOS DE SEGMENTO (ALERTAS)\n",
    "# ============================================\n",
    "\n",
    "# Identificar clientes que cambiaron de segmento\n",
    "cambios_segmento = spark.read.format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", version_before + 1) \\\n",
    "    .table(BRONZE_CLIENTES) \\\n",
    "    .filter(col(\"_change_type\").isin([\"update_preimage\", \"update_postimage\"]))\n",
    "\n",
    "# Comparar pre y post image para detectar cambios de segmento\n",
    "pre_image = cambios_segmento.filter(col(\"_change_type\") == \"update_preimage\") \\\n",
    "    .select(\n",
    "        col(\"cliente_id\"),\n",
    "        col(\"segmento_cliente\").alias(\"segmento_anterior\"),\n",
    "        col(\"_commit_version\")\n",
    "    )\n",
    "\n",
    "post_image = cambios_segmento.filter(col(\"_change_type\") == \"update_postimage\") \\\n",
    "    .select(\n",
    "        col(\"cliente_id\"),\n",
    "        col(\"segmento_cliente\").alias(\"segmento_nuevo\"),\n",
    "        col(\"nombre\"),\n",
    "        col(\"_commit_version\")\n",
    "    )\n",
    "\n",
    "alertas_segmento = pre_image.join(\n",
    "    post_image,\n",
    "    [\"cliente_id\", \"_commit_version\"]\n",
    ").filter(\n",
    "    col(\"segmento_anterior\") != col(\"segmento_nuevo\")\n",
    ")\n",
    "\n",
    "print(\"üîî ALERTAS: Cambios de Segmento Detectados\")\n",
    "alertas_segmento.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CASO 2: DETECTAR CAMBIOS DE DIRECCI√ìN (COMPLIANCE)\n",
    "# ============================================\n",
    "\n",
    "# Similar al anterior pero para direcci√≥n\n",
    "pre_direccion = cambios_segmento.filter(col(\"_change_type\") == \"update_preimage\") \\\n",
    "    .select(\n",
    "        col(\"cliente_id\"),\n",
    "        col(\"direccion\").alias(\"direccion_anterior\"),\n",
    "        col(\"ciudad\").alias(\"ciudad_anterior\"),\n",
    "        col(\"_commit_version\")\n",
    "    )\n",
    "\n",
    "post_direccion = cambios_segmento.filter(col(\"_change_type\") == \"update_postimage\") \\\n",
    "    .select(\n",
    "        col(\"cliente_id\"),\n",
    "        col(\"direccion\").alias(\"direccion_nueva\"),\n",
    "        col(\"ciudad\").alias(\"ciudad_nueva\"),\n",
    "        col(\"nombre\"),\n",
    "        col(\"_commit_version\"),\n",
    "        col(\"_commit_timestamp\")\n",
    "    )\n",
    "\n",
    "cambios_direccion = pre_direccion.join(\n",
    "    post_direccion,\n",
    "    [\"cliente_id\", \"_commit_version\"]\n",
    ").filter(\n",
    "    (col(\"direccion_anterior\") != col(\"direccion_nueva\")) |\n",
    "    (col(\"ciudad_anterior\") != col(\"ciudad_nueva\"))\n",
    ")\n",
    "\n",
    "print(\"üìç Cambios de Direcci√≥n (para Compliance KYC):\")\n",
    "cambios_direccion.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 M√©tricas de Cambios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# M√âTRICAS DE CDC\n",
    "# ============================================\n",
    "\n",
    "# Leer todo el historial de CDC\n",
    "all_changes = spark.read.format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", 0) \\\n",
    "    .table(BRONZE_CLIENTES)\n",
    "\n",
    "print(\"üìä M√©tricas de Change Data Feed:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Cambios por tipo\n",
    "print(\"\\nüìà Cambios por tipo:\")\n",
    "all_changes.groupBy(\"_change_type\").count() \\\n",
    "    .withColumnRenamed(\"count\", \"total\") \\\n",
    "    .orderBy(\"_change_type\").show()\n",
    "\n",
    "# Cambios por versi√≥n\n",
    "print(\"\\nüìà Cambios por versi√≥n:\")\n",
    "all_changes.groupBy(\"_commit_version\").count() \\\n",
    "    .withColumnRenamed(\"count\", \"total_cambios\") \\\n",
    "    .orderBy(\"_commit_version\").show(10)\n",
    "\n",
    "# Timeline de cambios\n",
    "print(\"\\nüìà Timeline de cambios por hora:\")\n",
    "all_changes.withColumn(\n",
    "    \"hora\", date_trunc(\"hour\", col(\"_commit_timestamp\"))\n",
    ").groupBy(\"hora\", \"_change_type\").count() \\\n",
    "    .orderBy(\"hora\", \"_change_type\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Funci√≥n Reutilizable para Procesar CDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FUNCI√ìN GEN√âRICA PARA PROCESAR CDC\n",
    "# ============================================\n",
    "\n",
    "def process_cdc_batch(\n",
    "    source_table: str,\n",
    "    start_version: int = None,\n",
    "    end_version: int = None,\n",
    "    start_timestamp: str = None,\n",
    "    end_timestamp: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Procesa Change Data Feed de una tabla Delta.\n",
    "    \n",
    "    Args:\n",
    "        source_table: Nombre de la tabla fuente\n",
    "        start_version: Versi√≥n inicial (opcional)\n",
    "        end_version: Versi√≥n final (opcional)\n",
    "        start_timestamp: Timestamp inicial (opcional)\n",
    "        end_timestamp: Timestamp final (opcional)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con los cambios y columnas metadata de CDC\n",
    "    \"\"\"\n",
    "    reader = spark.read.format(\"delta\").option(\"readChangeFeed\", \"true\")\n",
    "    \n",
    "    # Configurar rango por versi√≥n\n",
    "    if start_version is not None:\n",
    "        reader = reader.option(\"startingVersion\", start_version)\n",
    "    if end_version is not None:\n",
    "        reader = reader.option(\"endingVersion\", end_version)\n",
    "    \n",
    "    # Configurar rango por timestamp\n",
    "    if start_timestamp is not None:\n",
    "        reader = reader.option(\"startingTimestamp\", start_timestamp)\n",
    "    if end_timestamp is not None:\n",
    "        reader = reader.option(\"endingTimestamp\", end_timestamp)\n",
    "    \n",
    "    return reader.table(source_table)\n",
    "\n",
    "def get_only_latest_changes(cdc_df, key_columns: list):\n",
    "    \"\"\"\n",
    "    De un DataFrame de CDC, obtiene solo el estado m√°s reciente de cada registro.\n",
    "    √ötil para sincronizaci√≥n de tablas.\n",
    "    \n",
    "    Args:\n",
    "        cdc_df: DataFrame con datos de CDC\n",
    "        key_columns: Lista de columnas que forman la clave\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con solo el estado m√°s reciente de cada registro\n",
    "    \"\"\"\n",
    "    from pyspark.sql.window import Window\n",
    "    \n",
    "    # Solo tomar inserts y post-images (estado actual)\n",
    "    current_state = cdc_df.filter(\n",
    "        col(\"_change_type\").isin([\"insert\", \"update_postimage\"])\n",
    "    )\n",
    "    \n",
    "    # Ordenar por versi√≥n y tomar el m√°s reciente\n",
    "    window = Window.partitionBy(*key_columns).orderBy(col(\"_commit_version\").desc())\n",
    "    \n",
    "    return current_state.withColumn(\n",
    "        \"rn\", row_number().over(window)\n",
    "    ).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "print(\"‚úÖ Funciones de utilidad para CDC definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso de las funciones\n",
    "cdc_data = process_cdc_batch(BRONZE_CLIENTES, start_version=0)\n",
    "latest_state = get_only_latest_changes(cdc_data, [\"cliente_id\"])\n",
    "\n",
    "print(f\"üìä Total cambios en CDC: {cdc_data.count()}\")\n",
    "print(f\"üìä Registros √∫nicos (estado actual): {latest_state.count()}\")\n",
    "\n",
    "# Mostrar algunos registros\n",
    "latest_state.select(\n",
    "    \"cliente_id\", \"nombre\", \"segmento_cliente\", \"estado\", \"_change_type\", \"_commit_version\"\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Verificar Historial de la Tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VER HISTORIAL COMPLETO\n",
    "# ============================================\n",
    "\n",
    "print(f\"üìú Historial de {BRONZE_CLIENTES}:\")\n",
    "spark.sql(f\"DESCRIBE HISTORY {BRONZE_CLIENTES}\").select(\n",
    "    \"version\",\n",
    "    \"timestamp\",\n",
    "    \"operation\",\n",
    "    \"operationParameters\",\n",
    "    \"operationMetrics\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Resumen del Notebook\n",
    "\n",
    "### Lo que aprendimos:\n",
    "\n",
    "1. ‚úÖ **Habilitar CDC** en tablas Delta existentes\n",
    "2. ‚úÖ **Simular operaciones** (INSERT, UPDATE, DELETE)\n",
    "3. ‚úÖ **Leer Change Data Feed** en modo batch y streaming\n",
    "4. ‚úÖ **Analizar los tipos de cambio** (insert, update_preimage, update_postimage, delete)\n",
    "5. ‚úÖ **Casos de uso pr√°cticos**: Alertas, compliance, m√©tricas\n",
    "6. ‚úÖ **Funciones reutilizables** para procesar CDC\n",
    "\n",
    "### Columnas Metadata de CDC:\n",
    "\n",
    "| Columna | Descripci√≥n |\n",
    "|---------|-------------|\n",
    "| `_change_type` | Tipo de cambio (insert, update_preimage, update_postimage, delete) |\n",
    "| `_commit_version` | Versi√≥n del commit en Delta Lake |\n",
    "| `_commit_timestamp` | Timestamp del commit |\n",
    "\n",
    "### Pr√≥ximo paso:\n",
    "Continuar con el **Notebook 03: Silver Layer - SCD** para implementar Slowly Changing Dimensions.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš¡ Notebook 05: Liquid Clustering - OptimizaciÃ³n Avanzada\n",
    "\n",
    "## Objetivo\n",
    "Explorar y demostrar **Liquid Clustering** en Azure Databricks:\n",
    "- QuÃ© es y cÃ³mo funciona\n",
    "- ComparaciÃ³n con Partitioning y Z-Ordering tradicionales\n",
    "- Benchmarks de rendimiento\n",
    "- Mejores prÃ¡cticas y cuÃ¡ndo usarlo\n",
    "\n",
    "## Â¿QuÃ© es Liquid Clustering?\n",
    "\n",
    "**Liquid Clustering** es una tÃ©cnica de optimizaciÃ³n de datos en Delta Lake que:\n",
    "\n",
    "| CaracterÃ­stica | DescripciÃ³n |\n",
    "|----------------|-------------|\n",
    "| **Flexible** | Puedes cambiar las columnas de clustering sin reescribir toda la tabla |\n",
    "| **Incremental** | Solo optimiza los datos nuevos/modificados |\n",
    "| **AutomÃ¡tico** | Se integra con `OPTIMIZE` para mantenimiento continuo |\n",
    "| **Sin particiones** | Elimina la necesidad de elegir particiones estÃ¡ticas |\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    LIQUID CLUSTERING                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   Antes (Partitioning):          DespuÃ©s (Liquid Clustering):   â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\n",
    "â”‚   â”‚ /date=2024-01/  â”‚            â”‚ Datos organizados            â”‚\n",
    "â”‚   â”‚   /region=US/   â”‚    â”€â”€â–º     â”‚ dinÃ¡micamente por            â”‚\n",
    "â”‚   â”‚     file1.parquetâ”‚            â”‚ CLUSTER BY (col1, col2)     â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚                              â”‚\n",
    "â”‚   âš ï¸ Estructura rÃ­gida           â”‚ âœ… Flexible y eficiente      â”‚\n",
    "â”‚   âš ï¸ Small files problem         â”‚ âœ… Sin small files           â”‚\n",
    "â”‚   âš ï¸ DifÃ­cil de cambiar          â”‚ âœ… FÃ¡cil de modificar        â”‚\n",
    "â”‚                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ConfiguraciÃ³n y VerificaciÃ³n de Compatibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar versiÃ³n de Databricks Runtime\n",
    "import os\n",
    "\n",
    "# Obtener informaciÃ³n del cluster\n",
    "spark_version = spark.version\n",
    "dbr_version = spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\", \"Unknown\")\n",
    "\n",
    "print(\"ğŸ” VERIFICACIÃ“N DE COMPATIBILIDAD\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Spark Version: {spark_version}\")\n",
    "print(f\"Databricks Runtime: {dbr_version}\")\n",
    "\n",
    "# Liquid Clustering requiere DBR 13.3+ para creaciÃ³n y DBR 15.2+ para clustering manual\n",
    "print(\"\\nğŸ“‹ Requisitos de Liquid Clustering:\")\n",
    "print(\"   â€¢ DBR 13.3+: Crear tablas con CLUSTER BY\")\n",
    "print(\"   â€¢ DBR 14.1+: OptimizaciÃ³n mejorada\")\n",
    "print(\"   â€¢ DBR 15.2+: Clustering manual en Standard tier\")\n",
    "print(\"\\nâœ… Azure Databricks Standard tier: SOPORTADO (DBR 15.2+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfiguraciÃ³n\n",
    "DATABASE_NAME = \"financial_lakehouse\"\n",
    "spark.sql(f\"USE {DATABASE_NAME}\")\n",
    "\n",
    "# Habilitar optimizaciones\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "\n",
    "print(f\"âœ… Base de datos: {DATABASE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. ComparaciÃ³n: Partitioning vs Z-Order vs Liquid Clustering\n",
    "\n",
    "Vamos a crear tres versiones de la misma tabla para comparar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos de prueba mÃ¡s grandes para benchmarks\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import random\n",
    "\n",
    "print(\"ğŸ”„ Generando datos de prueba para benchmarks...\")\n",
    "\n",
    "# Generar 1 millÃ³n de transacciones para pruebas de rendimiento\n",
    "num_records = 1000000\n",
    "\n",
    "# Crear DataFrame con datos sintÃ©ticos\n",
    "df_benchmark = (\n",
    "    spark.range(0, num_records)\n",
    "    .withColumn(\"transaction_id\", concat(lit(\"TXN-\"), format_string(\"%010d\", col(\"id\"))))\n",
    "    .withColumn(\"fecha\", date_sub(current_date(), (rand() * 365).cast(\"int\")))\n",
    "    .withColumn(\"cliente_id\", concat(lit(\"CLI-\"), format_string(\"%06d\", (rand() * 10000).cast(\"int\"))))\n",
    "    .withColumn(\"cuenta_id\", concat(lit(\"CTA-\"), format_string(\"%08d\", (rand() * 15000).cast(\"int\"))))\n",
    "    .withColumn(\"tipo_transaccion\", \n",
    "        when(rand() < 0.3, \"DEPOSITO\")\n",
    "        .when(rand() < 0.5, \"RETIRO\")\n",
    "        .when(rand() < 0.7, \"TRANSFERENCIA\")\n",
    "        .otherwise(\"PAGO\")\n",
    "    )\n",
    "    .withColumn(\"monto\", round(rand() * 10000 + 10, 2))\n",
    "    .withColumn(\"region\", \n",
    "        when(rand() < 0.25, \"NORTE\")\n",
    "        .when(rand() < 0.50, \"SUR\")\n",
    "        .when(rand() < 0.75, \"ESTE\")\n",
    "        .otherwise(\"OESTE\")\n",
    "    )\n",
    "    .withColumn(\"sucursal\", concat(lit(\"SUC-\"), format_string(\"%03d\", (rand() * 100).cast(\"int\"))))\n",
    "    .withColumn(\"canal\", \n",
    "        when(rand() < 0.4, \"APP_MOVIL\")\n",
    "        .when(rand() < 0.7, \"WEB\")\n",
    "        .when(rand() < 0.9, \"ATM\")\n",
    "        .otherwise(\"SUCURSAL\")\n",
    "    )\n",
    "    .withColumn(\"estado\", \n",
    "        when(rand() < 0.95, \"COMPLETADA\")\n",
    "        .when(rand() < 0.98, \"PENDIENTE\")\n",
    "        .otherwise(\"FALLIDA\")\n",
    "    )\n",
    "    .drop(\"id\")\n",
    ")\n",
    "\n",
    "# Cache para reutilizar\n",
    "df_benchmark.cache()\n",
    "count = df_benchmark.count()\n",
    "\n",
    "print(f\"âœ… Generados {count:,} registros para benchmarks\")\n",
    "df_benchmark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tabla con Partitioning Tradicional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# OpciÃ³n 1: Partitioning tradicional\n",
    "print(\"ğŸ“ Creando tabla con PARTITIONING tradicional...\")\n",
    "\n",
    "# Eliminar si existe\n",
    "spark.sql(\"DROP TABLE IF EXISTS benchmark_partitioned\")\n",
    "\n",
    "# Crear tabla particionada por fecha y regiÃ³n\n",
    "(\n",
    "    df_benchmark\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"fecha\", \"region\")\n",
    "    .saveAsTable(\"benchmark_partitioned\")\n",
    ")\n",
    "\n",
    "print(\"âœ… Tabla benchmark_partitioned creada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tabla con Z-Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# OpciÃ³n 2: Sin particiones pero con Z-Order\n",
    "print(\"ğŸ“ Creando tabla con Z-ORDER...\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS benchmark_zorder\")\n",
    "\n",
    "# Crear tabla sin particiones\n",
    "(\n",
    "    df_benchmark\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"benchmark_zorder\")\n",
    ")\n",
    "\n",
    "# Aplicar Z-Order despuÃ©s\n",
    "spark.sql(\"OPTIMIZE benchmark_zorder ZORDER BY (fecha, cliente_id, region)\")\n",
    "\n",
    "print(\"âœ… Tabla benchmark_zorder creada y optimizada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tabla con Liquid Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# OpciÃ³n 3: Liquid Clustering\n",
    "print(\"ğŸ“ Creando tabla con LIQUID CLUSTERING...\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS benchmark_liquid\")\n",
    "\n",
    "# Crear tabla con Liquid Clustering\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE benchmark_liquid (\n",
    "        transaction_id STRING,\n",
    "        fecha DATE,\n",
    "        cliente_id STRING,\n",
    "        cuenta_id STRING,\n",
    "        tipo_transaccion STRING,\n",
    "        monto DECIMAL(18,2),\n",
    "        region STRING,\n",
    "        sucursal STRING,\n",
    "        canal STRING,\n",
    "        estado STRING\n",
    "    )\n",
    "    USING DELTA\n",
    "    CLUSTER BY (fecha, cliente_id, region)\n",
    "\"\"\")\n",
    "\n",
    "# Insertar datos\n",
    "df_benchmark.write.format(\"delta\").mode(\"append\").saveAsTable(\"benchmark_liquid\")\n",
    "\n",
    "# Ejecutar OPTIMIZE para aplicar clustering\n",
    "spark.sql(\"OPTIMIZE benchmark_liquid\")\n",
    "\n",
    "print(\"âœ… Tabla benchmark_liquid creada y optimizada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar tamaÃ±os y estructura de archivos\n",
    "print(\"ğŸ“Š COMPARACIÃ“N DE ESTRUCTURAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for tabla in [\"benchmark_partitioned\", \"benchmark_zorder\", \"benchmark_liquid\"]:\n",
    "    print(f\"\\nğŸ“¦ {tabla}\")\n",
    "    \n",
    "    # Obtener detalles\n",
    "    detail = spark.sql(f\"DESCRIBE DETAIL {tabla}\").collect()[0]\n",
    "    \n",
    "    print(f\"   TamaÃ±o: {detail['sizeInBytes'] / (1024*1024):.2f} MB\")\n",
    "    print(f\"   Archivos: {detail['numFiles']}\")\n",
    "    \n",
    "    # Verificar propiedades de clustering\n",
    "    props = spark.sql(f\"SHOW TBLPROPERTIES {tabla}\").collect()\n",
    "    clustering_cols = None\n",
    "    for prop in props:\n",
    "        if 'clusteringColumns' in prop['key']:\n",
    "            clustering_cols = prop['value']\n",
    "            break\n",
    "    \n",
    "    if clustering_cols:\n",
    "        print(f\"   Liquid Clustering: {clustering_cols}\")\n",
    "    \n",
    "    # Verificar particiones\n",
    "    if detail['partitionColumns']:\n",
    "        print(f\"   Particiones: {detail['partitionColumns']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Benchmarks de Rendimiento\n",
    "\n",
    "Ejecutamos las mismas consultas en las tres tablas para comparar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "def benchmark_query(query: str, tabla: str, num_runs: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Ejecuta una query mÃºltiples veces y retorna estadÃ­sticas.\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    rows_scanned = 0\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        # Limpiar cache\n",
    "        spark.catalog.clearCache()\n",
    "        \n",
    "        # Ejecutar query\n",
    "        start = time.time()\n",
    "        result = spark.sql(query.format(tabla=tabla))\n",
    "        count = result.count()  # Forzar ejecuciÃ³n\n",
    "        end = time.time()\n",
    "        \n",
    "        times.append(end - start)\n",
    "        rows_scanned = count\n",
    "    \n",
    "    return {\n",
    "        'tabla': tabla,\n",
    "        'min_time': min(times),\n",
    "        'max_time': max(times),\n",
    "        'avg_time': sum(times) / len(times),\n",
    "        'rows': rows_scanned\n",
    "    }\n",
    "\n",
    "def run_benchmark_suite(queries: List[Dict], tablas: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Ejecuta suite de benchmarks y muestra resultados.\n",
    "    \"\"\"\n",
    "    for query_info in queries:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ğŸ” {query_info['name']}\")\n",
    "        print(f\"   Query: {query_info['query'][:80]}...\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        results = []\n",
    "        for tabla in tablas:\n",
    "            result = benchmark_query(query_info['query'], tabla)\n",
    "            results.append(result)\n",
    "            print(f\"\\n   ğŸ“¦ {tabla}\")\n",
    "            print(f\"      Tiempo promedio: {result['avg_time']:.3f}s\")\n",
    "            print(f\"      Rango: {result['min_time']:.3f}s - {result['max_time']:.3f}s\")\n",
    "        \n",
    "        # Comparar\n",
    "        best = min(results, key=lambda x: x['avg_time'])\n",
    "        print(f\"\\n   ğŸ† Mejor: {best['tabla']} ({best['avg_time']:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir queries de benchmark\n",
    "benchmark_queries = [\n",
    "    {\n",
    "        'name': 'Query 1: Filtro por fecha especÃ­fica',\n",
    "        'query': \"SELECT * FROM {tabla} WHERE fecha = '2024-06-15'\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'Query 2: Filtro por rango de fechas',\n",
    "        'query': \"SELECT * FROM {tabla} WHERE fecha BETWEEN '2024-01-01' AND '2024-03-31'\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'Query 3: Filtro por cliente especÃ­fico',\n",
    "        'query': \"SELECT * FROM {tabla} WHERE cliente_id = 'CLI-001234'\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'Query 4: Filtro combinado (fecha + regiÃ³n)',\n",
    "        'query': \"SELECT * FROM {tabla} WHERE fecha >= '2024-06-01' AND region = 'NORTE'\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'Query 5: AgregaciÃ³n por fecha y regiÃ³n',\n",
    "        'query': \"SELECT fecha, region, SUM(monto) as total, COUNT(*) as txns FROM {tabla} WHERE fecha >= '2024-01-01' GROUP BY fecha, region\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'Query 6: Filtro multi-columna (fecha + cliente + regiÃ³n)',\n",
    "        'query': \"SELECT * FROM {tabla} WHERE fecha >= '2024-06-01' AND cliente_id LIKE 'CLI-00%' AND region IN ('NORTE', 'SUR')\"\n",
    "    }\n",
    "]\n",
    "\n",
    "tablas_benchmark = [\n",
    "    'benchmark_partitioned',\n",
    "    'benchmark_zorder', \n",
    "    'benchmark_liquid'\n",
    "]\n",
    "\n",
    "print(\"ğŸš€ INICIANDO BENCHMARKS\")\n",
    "print(\"   Cada query se ejecuta 3 veces para promediar\\n\")\n",
    "\n",
    "run_benchmark_suite(benchmark_queries, tablas_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis de Data Skipping (archivos leÃ­dos)\n",
    "print(\"\\nğŸ“Š ANÃLISIS DE DATA SKIPPING\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nMenor nÃºmero de archivos leÃ­dos = mejor data skipping\\n\")\n",
    "\n",
    "test_query = \"SELECT COUNT(*) FROM {tabla} WHERE fecha = '2024-06-15' AND cliente_id = 'CLI-001234'\"\n",
    "\n",
    "for tabla in tablas_benchmark:\n",
    "    # Limpiar cache\n",
    "    spark.catalog.clearCache()\n",
    "    \n",
    "    # Ejecutar con explain\n",
    "    print(f\"\\nğŸ“¦ {tabla}:\")\n",
    "    spark.sql(test_query.format(tabla=tabla)).explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. CaracterÃ­sticas Avanzadas de Liquid Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Cambiar Clustering Keys (Sin Reescribir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una de las mayores ventajas: cambiar clustering keys dinÃ¡micamente\n",
    "print(\"ğŸ”„ CAMBIO DINÃMICO DE CLUSTERING KEYS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ver clustering actual\n",
    "print(\"\\n1ï¸âƒ£ Clustering actual:\")\n",
    "display(spark.sql(\"DESCRIBE DETAIL benchmark_liquid\").select(\"clusteringColumns\"))\n",
    "\n",
    "# Cambiar clustering keys\n",
    "print(\"\\n2ï¸âƒ£ Cambiando a CLUSTER BY (tipo_transaccion, canal, fecha)...\")\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE benchmark_liquid \n",
    "    CLUSTER BY (tipo_transaccion, canal, fecha)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ Nuevo clustering (sin reescribir datos):\")\n",
    "display(spark.sql(\"DESCRIBE DETAIL benchmark_liquid\").select(\"clusteringColumns\"))\n",
    "\n",
    "print(\"\\nâœ… Â¡Las keys se cambiaron instantÃ¡neamente!\")\n",
    "print(\"   Los datos existentes se re-clusterizan gradualmente con OPTIMIZE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar OPTIMIZE para re-clusterizar con las nuevas keys\n",
    "print(\"ğŸ”„ Aplicando OPTIMIZE para re-clusterizar...\")\n",
    "\n",
    "result = spark.sql(\"OPTIMIZE benchmark_liquid\")\n",
    "display(result)\n",
    "\n",
    "print(\"\\nâœ… Datos re-clusterizados con las nuevas keys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restaurar clustering original para el proyecto\n",
    "print(\"ğŸ”„ Restaurando clustering original...\")\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE benchmark_liquid \n",
    "    CLUSTER BY (fecha, cliente_id, region)\n",
    "\"\"\")\n",
    "spark.sql(\"OPTIMIZE benchmark_liquid\")\n",
    "print(\"âœ… Clustering restaurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Eliminar Clustering (CLUSTER BY NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla temporal para demostrar\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE demo_clustering_removal\n",
    "    CLUSTER BY (fecha, region)\n",
    "    AS SELECT * FROM benchmark_liquid LIMIT 10000\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ“¦ Tabla con clustering:\")\n",
    "display(spark.sql(\"DESCRIBE DETAIL demo_clustering_removal\").select(\"clusteringColumns\"))\n",
    "\n",
    "# Eliminar clustering\n",
    "print(\"\\nğŸ—‘ï¸ Eliminando clustering...\")\n",
    "spark.sql(\"ALTER TABLE demo_clustering_removal CLUSTER BY NONE\")\n",
    "\n",
    "print(\"\\nğŸ“¦ Tabla sin clustering:\")\n",
    "display(spark.sql(\"DESCRIBE DETAIL demo_clustering_removal\").select(\"clusteringColumns\"))\n",
    "\n",
    "# Limpiar\n",
    "spark.sql(\"DROP TABLE demo_clustering_removal\")\n",
    "print(\"\\nâœ… Demo completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Convertir Tabla Existente a Liquid Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla sin clustering\n",
    "print(\"ğŸ“¦ Creando tabla SIN clustering...\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE demo_conversion\n",
    "    AS SELECT * FROM benchmark_liquid LIMIT 50000\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n   Estado inicial:\")\n",
    "display(spark.sql(\"DESCRIBE DETAIL demo_conversion\").select(\"clusteringColumns\"))\n",
    "\n",
    "# Agregar Liquid Clustering a tabla existente\n",
    "print(\"\\nğŸ”„ Agregando Liquid Clustering...\")\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE demo_conversion\n",
    "    CLUSTER BY (fecha, cliente_id)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n   DespuÃ©s de ALTER:\")\n",
    "display(spark.sql(\"DESCRIBE DETAIL demo_conversion\").select(\"clusteringColumns\"))\n",
    "\n",
    "# OPTIMIZE para aplicar clustering\n",
    "print(\"\\nğŸ”„ Aplicando OPTIMIZE...\")\n",
    "spark.sql(\"OPTIMIZE demo_conversion\")\n",
    "\n",
    "print(\"\\nâœ… Tabla convertida exitosamente a Liquid Clustering\")\n",
    "\n",
    "# Limpiar\n",
    "spark.sql(\"DROP TABLE demo_conversion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Mejores PrÃ¡cticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejores prÃ¡cticas documentadas\n",
    "print(\"\"\"\n",
    "ğŸ“š MEJORES PRÃCTICAS PARA LIQUID CLUSTERING\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "1ï¸âƒ£  SELECCIÃ“N DE COLUMNAS DE CLUSTERING\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   âœ… Elegir columnas que se usen frecuentemente en WHERE\n",
    "   âœ… Priorizar columnas con alta cardinalidad\n",
    "   âœ… MÃ¡ximo 4 columnas (orden importa)\n",
    "   âŒ Evitar columnas que cambian constantemente\n",
    "   âŒ No usar columnas con muy baja cardinalidad (<100 valores)\n",
    "\n",
    "   Ejemplo Ã³ptimo para transacciones financieras:\n",
    "   CLUSTER BY (fecha, cliente_id, tipo_transaccion)\n",
    "                 â”‚          â”‚              â”‚\n",
    "                 â”‚          â”‚              â””â”€ Filtros por tipo\n",
    "                 â”‚          â””â”€ BÃºsquedas por cliente\n",
    "                 â””â”€ Rangos de fecha (mÃ¡s comÃºn)\n",
    "\n",
    "2ï¸âƒ£  CUÃNDO USAR LIQUID CLUSTERING vs PARTITIONING\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   \n",
    "   LIQUID CLUSTERING es mejor cuando:\n",
    "   âœ… Queries filtran por mÃºltiples columnas\n",
    "   âœ… Los patrones de consulta pueden cambiar\n",
    "   âœ… Tablas < 10TB\n",
    "   âœ… No quieres gestionar particiones manualmente\n",
    "   \n",
    "   PARTITIONING tradicional es mejor cuando:\n",
    "   âœ… Siempre filtras por la misma columna (ej: fecha)\n",
    "   âœ… Tablas muy grandes (>10TB)\n",
    "   âœ… Necesitas eliminar particiones completas frecuentemente\n",
    "   âœ… Tienes restricciones de retention por particiÃ³n\n",
    "\n",
    "3ï¸âƒ£  MANTENIMIENTO Y OPTIMIZE\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   âœ… Ejecutar OPTIMIZE regularmente (diario o despuÃ©s de ETL)\n",
    "   âœ… Usar Predictive Optimization si disponible (Unity Catalog)\n",
    "   âœ… Monitorear con DESCRIBE HISTORY\n",
    "   \n",
    "   -- OPTIMIZE bÃ¡sico\n",
    "   OPTIMIZE nombre_tabla\n",
    "   \n",
    "   -- OPTIMIZE con WHERE para tablas grandes\n",
    "   OPTIMIZE nombre_tabla WHERE fecha >= current_date() - 7\n",
    "\n",
    "4ï¸âƒ£  NO MEZCLAR CON PARTITIONING\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   âŒ No usar PARTITION BY junto con CLUSTER BY\n",
    "   âŒ Convertir tablas particionadas requiere reescritura\n",
    "\n",
    "5ï¸âƒ£  LÃMITES Y CONSIDERACIONES\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   â€¢ MÃ¡ximo 4 columnas de clustering\n",
    "   â€¢ Compatible con DBR 13.3+\n",
    "   â€¢ Clustering manual requiere DBR 15.2+ (Standard tier)\n",
    "   â€¢ Auto-clustering (Predictive Optimization) requiere Unity Catalog\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. AplicaciÃ³n a las Tablas del Proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisar clustering en las tablas del proyecto\n",
    "print(\"ğŸ“Š LIQUID CLUSTERING EN TABLAS DEL PROYECTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tablas_proyecto = [\n",
    "    (\"bronze_clientes\", \"CDC de clientes\"),\n",
    "    (\"bronze_cuentas\", \"CDC de cuentas\"),\n",
    "    (\"bronze_transacciones\", \"CDC de transacciones\"),\n",
    "    (\"silver_dim_clientes\", \"DimensiÃ³n SCD Type 2\"),\n",
    "    (\"silver_dim_cuentas\", \"DimensiÃ³n SCD HÃ­brida\"),\n",
    "    (\"gold_fact_transacciones_diarias\", \"Fact Table\"),\n",
    "    (\"gold_kpi_clientes\", \"KPIs de clientes\"),\n",
    "    (\"gold_kpi_cuentas\", \"KPIs de cuentas\"),\n",
    "    (\"gold_metricas_diarias\", \"MÃ©tricas diarias\")\n",
    "]\n",
    "\n",
    "for tabla, descripcion in tablas_proyecto:\n",
    "    try:\n",
    "        detail = spark.sql(f\"DESCRIBE DETAIL {tabla}\").collect()[0]\n",
    "        clustering = detail.get('clusteringColumns', [])\n",
    "        \n",
    "        print(f\"\\nğŸ“¦ {tabla}\")\n",
    "        print(f\"   â””â”€ {descripcion}\")\n",
    "        if clustering:\n",
    "            print(f\"   â””â”€ Clustering: {clustering}\")\n",
    "        else:\n",
    "            print(f\"   â””â”€ Clustering: No configurado\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ {tabla}: No encontrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunciÃ³n para optimizar todas las tablas del proyecto\n",
    "def optimize_project_tables():\n",
    "    \"\"\"\n",
    "    Ejecuta OPTIMIZE en todas las tablas del proyecto con Liquid Clustering.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ OPTIMIZANDO TABLAS DEL PROYECTO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    tablas = [\n",
    "        \"bronze_clientes\",\n",
    "        \"bronze_cuentas\", \n",
    "        \"bronze_transacciones\",\n",
    "        \"silver_dim_clientes\",\n",
    "        \"silver_dim_cuentas\",\n",
    "        \"gold_fact_transacciones_diarias\",\n",
    "        \"gold_kpi_clientes\",\n",
    "        \"gold_kpi_cuentas\",\n",
    "        \"gold_metricas_diarias\"\n",
    "    ]\n",
    "    \n",
    "    for tabla in tablas:\n",
    "        try:\n",
    "            print(f\"\\nğŸ”„ Optimizando {tabla}...\")\n",
    "            result = spark.sql(f\"OPTIMIZE {tabla}\")\n",
    "            metrics = result.collect()[0]\n",
    "            print(f\"   âœ… Archivos reescritos: {metrics.get('numFilesAdded', 0)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Error: {str(e)[:50]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… OptimizaciÃ³n completada\")\n",
    "\n",
    "# Ejecutar optimizaciÃ³n (comentado para no ejecutar automÃ¡ticamente)\n",
    "# optimize_project_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Monitoreo de Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunciÃ³n para monitorear estado de clustering\n",
    "def check_clustering_status(tabla: str):\n",
    "    \"\"\"\n",
    "    Muestra el estado de clustering de una tabla.\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“Š ESTADO DE CLUSTERING: {tabla}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Detalles de la tabla\n",
    "    detail = spark.sql(f\"DESCRIBE DETAIL {tabla}\").collect()[0]\n",
    "    \n",
    "    print(f\"\\nğŸ“¦ InformaciÃ³n General:\")\n",
    "    print(f\"   TamaÃ±o: {detail['sizeInBytes'] / (1024*1024):.2f} MB\")\n",
    "    print(f\"   Archivos: {detail['numFiles']}\")\n",
    "    print(f\"   Clustering Columns: {detail.get('clusteringColumns', 'N/A')}\")\n",
    "    \n",
    "    # Historial de operaciones OPTIMIZE\n",
    "    print(f\"\\nğŸ“œ Ãšltimas operaciones OPTIMIZE:\")\n",
    "    history = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            version,\n",
    "            timestamp,\n",
    "            operation,\n",
    "            operationMetrics.numAddedFiles as archivos_agregados,\n",
    "            operationMetrics.numRemovedFiles as archivos_eliminados\n",
    "        FROM (DESCRIBE HISTORY {tabla})\n",
    "        WHERE operation = 'OPTIMIZE'\n",
    "        ORDER BY version DESC\n",
    "        LIMIT 5\n",
    "    \"\"\")\n",
    "    display(history)\n",
    "\n",
    "# Ejemplo\n",
    "check_clustering_status(\"benchmark_liquid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Limpieza de Tablas de Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar tablas de benchmark (opcional)\n",
    "cleanup = False  # Cambiar a True para eliminar\n",
    "\n",
    "if cleanup:\n",
    "    print(\"ğŸ—‘ï¸ Limpiando tablas de benchmark...\")\n",
    "    \n",
    "    for tabla in ['benchmark_partitioned', 'benchmark_zorder', 'benchmark_liquid']:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {tabla}\")\n",
    "        print(f\"   âœ… {tabla} eliminada\")\n",
    "    \n",
    "    print(\"\\nâœ… Limpieza completada\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Tablas de benchmark conservadas para anÃ¡lisis adicional\")\n",
    "    print(\"   Cambiar 'cleanup = True' para eliminar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Œ Resumen: Liquid Clustering\n",
    "\n",
    "### Ventajas Principales\n",
    "\n",
    "| CaracterÃ­stica | Beneficio |\n",
    "|----------------|----------|\n",
    "| **Flexibilidad** | Cambiar clustering keys sin reescribir datos |\n",
    "| **Simplicidad** | No necesitas gestionar particiones |\n",
    "| **Data Skipping** | Mejor rendimiento en queries con filtros |\n",
    "| **Incremental** | OPTIMIZE solo procesa datos nuevos |\n",
    "| **Compatible** | Funciona con CDC y Time Travel |\n",
    "\n",
    "### CuÃ¡ndo Usar\n",
    "\n",
    "- âœ… Tablas con mÃºltiples patrones de consulta\n",
    "- âœ… Datos que crecen incrementalmente\n",
    "- âœ… Cuando no estÃ¡s seguro de las mejores particiones\n",
    "- âœ… Tablas medianas (<10TB)\n",
    "\n",
    "### Comandos Clave\n",
    "\n",
    "```sql\n",
    "-- Crear tabla con Liquid Clustering\n",
    "CREATE TABLE mi_tabla (...) CLUSTER BY (col1, col2, col3);\n",
    "\n",
    "-- Cambiar clustering keys\n",
    "ALTER TABLE mi_tabla CLUSTER BY (nueva_col1, nueva_col2);\n",
    "\n",
    "-- Eliminar clustering\n",
    "ALTER TABLE mi_tabla CLUSTER BY NONE;\n",
    "\n",
    "-- Aplicar/actualizar clustering\n",
    "OPTIMIZE mi_tabla;\n",
    "```\n",
    "\n",
    "---\n",
    "**Notebook 05 Completado** âœ…"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
